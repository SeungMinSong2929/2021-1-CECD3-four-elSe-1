{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.7.3 64-bit ('ProgramData': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from urllib import request\n",
    "import urllib\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./csv/침구-커튼-러그/러그.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"러그 feature.p\", \"rb\") as f:\n",
    "    inputs = pickle.load(f)\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx,input in enumerate(inputs):\n",
    "    if input.shape != (550,550,3):\n",
    "        del inputs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,input in enumerate(inputs):\n",
    "    if input.shape != (550,550,3):\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df[\"LV2_onehot\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = inputs[:3].copy()\n",
    "y = df[\"LV2_onehot\"][:3].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'\\xeb\\xb0\\x9c\\xeb\\xa7\\xa4\\xed\\x8a\\xb8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Encoder Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, (3, 3), padding=(1, 1))\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool1 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, (3, 3), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.maxpool2 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, (3, 3), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.maxpool3 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, 128, (3, 3), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.maxpool4 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, (3, 3), padding=(1, 1))\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.maxpool5 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Downscale the image with conv maxpool etc.\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.maxpool4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.maxpool5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Decoder Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, (2, 2), stride=(2, 2))\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, (2, 2), stride=(2, 2))\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, (2, 2), stride=(2, 2))\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 16, (2, 2), stride=(2, 2))\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv5 = nn.ConvTranspose2d(16, 3, (2, 2), stride=(2, 2))\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "         # Upscale the image with convtranspose etc.\n",
    "        x = self.deconv1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.deconv2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.deconv3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.deconv4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.deconv5(x)\n",
    "        x = self.relu5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "transforms = T.Compose([T.ToTensor()]) # Normalize the pixels and convert to tensor.\n",
    "\n",
    "full_dataset = URLDataset(df, transforms) # Create folder dataset.\n",
    "\n",
    "train_size = 0.75\n",
    "val_size = 1 - train_size\n",
    "\n",
    "# Split data to train and test\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size]) \n",
    "\n",
    "# Create the train dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    " \n",
    "# Create the validation dataloader\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Create the full dataloader\n",
    "full_loader = torch.utils.data.DataLoader(full_dataset, batch_size=32)\n",
    "\n",
    "loss_fn = nn.MSELoss() # We use Mean squared loss which computes difference between two images.\n",
    "\n",
    "encoder = ConvEncoder() # Our encoder model\n",
    "decoder = ConvDecoder() # Our decoder model\n",
    "\n",
    "device = \"cuda\"  # GPU device\n",
    "\n",
    "# Shift models to GPU\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Both the enocder and decoder parameters\n",
    "autoencoder_params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = optim.Adam(autoencoder_params, lr=1e-3) # Adam Optimizer\n",
    "\n",
    "# Time to Train !!!\n",
    "EPOCHS = 10\n",
    "# Usual Training Loop\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "        train_loss = train_step(encoder, decoder, train_loader, loss_fn, optimizer, device=device)\n",
    "        \n",
    "        print(f\"Epochs = {epoch}, Training Loss : {train_loss}\")\n",
    "        \n",
    "        val_loss = val_step(encoder, decoder, val_loader, loss_fn, device=device)\n",
    "        \n",
    "        print(f\"Epochs = {epoch}, Validation Loss : {val_loss}\")\n",
    "\n",
    "        # Simple Best Model saving\n",
    "        if val_loss < max_loss:\n",
    "            print(\"Validation Loss decreased, saving new best model\")\n",
    "            torch.save(encoder.state_dict(), \"encoder_model.pt\")\n",
    "            torch.save(decoder.state_dict(), \"decoder_model.pt\")\n",
    "\n",
    "# Save the feature representations.\n",
    "EMBEDDING_SHAPE = (1, 256, 16, 16) # This we know from our encoder\n",
    "\n",
    "# We need feature representations for complete dataset not just train and validation.\n",
    "# Hence we use full loader here.\n",
    "embedding = create_embedding(encoder, full_loader, EMBEDDING_SHAPE, device)\n",
    "\n",
    "# Convert embedding to numpy and save them\n",
    "numpy_embedding = embedding.cpu().detach().numpy()\n",
    "num_images = numpy_embedding.shape[0]\n",
    "\n",
    "# Save the embeddings for complete dataset, not just train\n",
    "flattened_embedding = numpy_embedding.reshape((num_images, -1))\n",
    "np.save(\"data_embedding.npy\", flattened_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}